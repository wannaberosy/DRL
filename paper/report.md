# 1.课题需求与国内外研究状况
## 课题需求
随着大型语言模型（LLM）的快速发展，它们在文本生成、问答等任务上取得了引人注目的成果。然而，现有的主流 LLM 缺乏真实的行动能力和规划能力：当模型面临需要多步骤推理、计划决策、工具调用或环境交互的复杂任务时，往往显得力不从心[1]。例如，在传统搜索和问答场景中，LLM 通常以左到右的链式推理方式工作，一步步生成答案。但这种线性推理模式在遇到复杂问题时容易陷入局部最优，缺乏全局探索能力。这导致模型可能产生幻觉或错误结论，无法保证推理过程的可靠性。
当前广泛采用的“检索增强生成”（RAG）等方案在一定程度上提升了信息获取效率，但它们的交互模式通常是静态单轮的[2]。LLM 在其中主要扮演“优化者”或“总结者”的角色，难以应对需要多步骤、动态调整的复杂调研任务[2]。因此，业界和学界认识到仅靠被动的单轮问答已无法满足更高层次智能体的需求，迫切需要让 LLM 从“回答问题”进化为“自主执行任务”[3]。具体而言，这意味着赋予模型以下能力：将任务分解为子目标并规划多步行动，在每一步决策中利用环境反馈进行调整，并能调用外部工具或检索外部知识来辅助决策。这种转变对于构建具备自主性的智能代理至关重要，是当前人工智能研究中最具挑战性的方向之一[3][4]。
为了解决上述需求，研究者开始探索将经典的决策搜索算法引入到 LLM 的推理过程中。其中，蒙特卡洛树搜索（MCTS）作为一种成熟的启发式搜索方法，因其在博弈决策领域的成功而备受关注。例如，DeepMind 团队的 AlphaGo 系统结合深度神经网络和 MCTS，在围棋任务上达到了划时代的表现，首次击败了人类职业选手[5]。MCTS 通过在状态空间中模拟多轮决策，能够有效平衡探索与利用，在复杂决策环境中找到高质量方案。因此，一个合理的设想是：将 MCTS 等搜索策略与 LLM 相结合，让语言模型具备类似游戏AI的规划搜索能力，从而能够在解答复杂问题时探索多条推理路径，避免过早收敛到错误答案[6][7]。这种方法有望显著提升 LLM 在推理任务中的可靠性和成功率。总之，从应用需求看，无论是在学术问题（如数学推导、复杂问答）还是实际场景（如自动驾驶决策、指挥控制等复杂任务）中，都迫切需要一种融合语言智能和搜索规划的新型方法，以突破当前 LLM 智能体在推理决策方面的瓶颈。
## 国外研究现状
在国际上，围绕LLM智能体推理与规划的研究近两年蓬勃兴起，涌现出一系列创新方法和框架。早期的工作侧重于增强语言模型的链式推理能力，例如提出思维链（Chain-of-Thought）提示技术，让模型在给出最终答案前生成一系列中间推理步骤，从而提高复杂问题的解答准确性[6]。基于此思路，自洽性推理（Self-Consistency）方法进一步通过对同一问题采样生成多条思维链并投票，来提升答案可靠度[8]。然而，这些方法仍局限于线性推理，无法在单次推理过程中灵活地探索不同路径。
2022年底，来自普林斯顿等机构的研究者提出了ReAct范式[9]。ReAct将“思考”和“行动”相结合，把LLM的推理过程显式地表示为交替的“Thought-Action-Observation”（思考-行动-观察）序列[9]。具体而言，模型首先生成思考内容（如推理分析），然后根据思考选择行动（如调用工具或检索知识），接着获得环境观察结果并反馈到下一步思考中。通过这种循环，ReAct实现了LLM与外部环境的互动，在开放域问答、知识检索等任务中显著提升了性能[9]。ReAct的优势在于：它赋予了模型一定的计划执行能力和可解释性，能够通过工具调用扩充模型的知识范围[9]。但其局限也很明显：ReAct采用的是贪心式的线性搜索策略，每次只沿着当前认为最优的路径向前推进，缺乏全局搜索视野。这使得模型可能在决策过程中过早地陷入某一路径，一旦该路径不能通向正确答案，就难以及时回溯和探索其他可能方向。
针对ReAct的局限，学者们开始尝试为LLM智能体引入非线性的决策探索机制。树形搜索理念逐渐进入视野：2023年，Princeton团队提出了思想树(Tree of Thoughts, ToT)方法[6]。ToT将Chain-of-Thought推广为树状结构，即模型在每个推理步骤产生多个候选“想法”（thought），形成分支，然后通过评估或启发式选择最优分支继续推进[10]。这种方法允许模型同时探索多种推理路线，并可在必要时进行回溯和重新选择，从而显著增强了解题的规划性。实验表明，Tree of Thoughts 能够大幅提升复杂推理任务的成功率。例如，在24点算术游戏中，GPT-4 结合思维树策略仅能解出4%的题目，而使用ToT框架后成功率飙升至74%[11]。这一结果充分证明了树式思维对提升LLM决策能力的价值。
与ToT类似，2023年出现的Reflexion框架引入了自我反思机制[12][13]。Reflexion并非直接采用搜索算法，而是让智能体在每轮尝试后根据反馈生成反思，将错误教训以语言形式存入记忆池，指导后续回合的决策[12][13]。通过这种语言层面的强化学习（不更新模型权重，而是靠文本反馈），Reflexion在多个任务上显著提高了LLM的成功率。例如，在HumanEval编程基准上，Reflexion使得GPT-4的单次通过率从80%提升到91%[14]。Reflexion展示了利用语言反馈实现强化决策的前景，与搜索方法形成了互补：前者强调通过多次试错学习策略，后者强调在一次推理中广泛探索。
在将搜索算法直接用于LLM推理方面，最具代表性的是语言智能体树搜索（Language Agent Tree Search, LATS）方法。Zhou 等人于2023年底在arXiv发表了相关工作[15]。LATS 可视作将 MCTS 完全集成到LLM智能体中的首个框架：它把LLM的思考-行动序列构造成一棵搜索树，树节点表示智能体在某一步的状态（包含累积的思考、行动和观察序列），树边表示可选的行动。在决策过程中，LATS 执行典型的MCTS四阶段：选择最优节点、扩展候选动作、由LLM对候选进行评估得到价值、然后模拟并回溯更新[16]。通过上置信上限(UCB)等策略，LATS 能够在庞大的决策空间中统计指导搜索方向，避免了盲目探索。与ReAct只能沿一条路走到黑不同，LATS 并行探索多个推理路径，大大降低了错失正确答案路径的风险[16]。此外，LATS 还融入了自我反思步骤：对于每个候选路径，模型可以结合自身观察和反馈，对推理过程进行检查，一旦发现谬误便尝试替代方案，从而提升决策的稳健性[17]。
实验研究表明，LATS 在一系列复杂推理任务中取得了优于传统方法的效果。以多跳问答为例，在 HotpotQA 等数据集上，相较于 ReAct 框架，LATS 在答案精准率和任务成功率上均有显著提升[18]。又比如在代码生成、交互式问答等需要多步决策的场景，LATS 能够更有效地规划步骤并避免错误[18]。需要指出的是，LATS 的高性能是以一定的计算代价为前提的：由于引入了树搜索和反思评估，计算开销和时间消耗较 ReAct 等线性方法有所增加[18]。然而在许多关键任务中，这一代价是值得的，因为 LATS 换来了更强的推理泛化能力和可靠性[18]。
除上述方法外，国际上还有一些相关探索值得关注。例如，Wang 等人提出的Tree-of-Thoughts 改进版引入了步骤级的Q值评估，通过强化学习训练模型学会评估每个推理步骤的价值，从而指导树搜索更高效地收敛[19]。OpenAI 在 GPT-4 的系统卡中也提出了一种“慢思考”机制，结合MCTS策略和验证模型来模拟人类的深思熟虑过程，提高推理准确性[20]。另外，2024年出现的R-Search框架[21]将检索式搜索与LLM推理相集成，通过多重奖励的强化学习使模型能在何时检索、何时推理之间动态决策，进一步提升了知识密集型任务中的推理表现[22]。总体而言，国外研究在过去两年中已经初步构建了LLM自主智能体的雏形，从利用环境反馈的ReAct到树搜索规划的LATS，再到自我改进的Reflexion，呈现出多方向并进的局面。这些方法大多在国际顶会上发表或报告（如 NeurIPS 2023、ICLR 2024 等），标志着LLM智能体正迅速成为人工智能领域的新热点。
值得一提的是，国外学者还开始对这一新兴领域进行系统性综述和评估框架的搭建。例如，2023年底的一篇综述详细分类了基于LLM的自主代理研究，涵盖了推理、规划、工具调用等模块[15]。2024年4月也有研究发布了关于AI Agent架构的综述，分析了单智能体和多智能体系统中的关键模式和差异[23]。2025年更有学者联合16家机构撰写了长达百页的Agentic RL综述，整合了500多篇相关工作，对具身智能体强化学习的概念、框架和应用进行了系统梳理[24]。这些综述工作的出现，表明国际研究者正试图提炼共性原理，指导未来研究。从综述观点看，一个共识是：未来的LLM智能体应当集成规划(Planning)、记忆(Memory)、工具使用(Tool Use)、推理(Reasoning)、自我反思(Self-Reflection)等多种能力[25][26]，通过强化学习等途径提升模型的自主决策水平[4]。

## 国内研究现状
近年来，国内研究者也紧跟国际前沿，在LLM智能体与树搜索增强方面开展了一系列工作。在总体思路上，国内研究与国际趋势一脉相承，同样关注如何让大型语言模型具备自主规划、多步推理的能力。一些中国团队在国际顶会上发表了相关成果，体现出不俗的竞争力。
首先，值得关注的是国内学者在将树搜索与LLM相结合方面所做的探索。中国人民大学等单位的研究团队提出了AR-MCTS框架[27]（已被ACL 2025接收）。AR-MCTS 的设计初衷是提升多模态大模型（MLLM）的复杂推理能力[28]。该框架巧妙地将主动检索（Active Retrieval, AR）与蒙特卡洛树搜索融合：在每一步推理中，智能体不仅利用自身语言模型产生候选推理步骤，还通过检索模块主动获取与当前问题相关的跨模态知识[28][27]。然后，AR-MCTS 使用 MCTS 策略在“候选步骤+检索信息”的扩展状态下进行模拟和评估，选出最优的下一步推理决策。通过不断迭代，AR-MCTS 能逐步产生高质量的多模态推理路径，并利用所累积的数据对过程奖励模型（PRM）进行强化训练，从而提升模型对复杂问题的理解和验证能力[29][30]。实验结果显示，AR-MCTS 框架在跨模态推理任务上显著优于传统方法，如在综合性高考多模态基准（GAOKAO-MM）上，相比零样本GPT-4V等模型取得了更高的准确度[31]。AR-MCTS 是国内学者将检索、推理、搜索三者相结合的一次成功实践，表明通过引入知识检索和树搜索可以有效增强大型模型在复杂场景下的推理表现。
除了在多模态领域的进展，国内研究者也在单模态的LLM智能体树搜索上有所贡献。微软亚洲研究院的团队提出了Tree-of-Thoughts 强化学习优化(TREE-GRPO)方法[32]。该方法针对LLM智能体在交互式环境中的决策优化问题，利用树搜索结构来提高强化学习的采样效率和稳定性。据报道，Tree-GRPO通过在训练过程中维护一棵搜索树来估计策略的价值，并使用树中节点的信息来指导策略梯度更新，从而相比传统策略优化方法获得更好的收敛效果[33]。虽然Tree-GRPO主要作为训练算法提出，但它与推理时的LATS框架理念相辅相成：前者优化了智能体的学习过程，后者提升了智能体的推理过程，两者结合有望进一步提高LLM智能体的性能。值得一提的是，该工作最初以预印本形式发表（2024年初[15]），其后续扩展在2025年继续完善[34]。这表明国内科研力量在这一领域正持续投入，并有能力引领某些技术方向。
在智能体推理的评测与应用方面，国内也开始逐步布局。一方面，综述和评论性工作已在国内出现，以促进学术交流。比如，上海交通大学团队于2025年发布了首篇LLM 搜索智能体综述[35]，系统分析了搜索智能体的范式、优化、应用和评估四个维度，为该领域提供了清晰的研究路线图[36][37]。该综述强调了从传统检索到自主搜索智能体的范式演变，指出未来搜索将从人工驱动转向AI主动信息获取的新阶段[38][39]。这种工作有助于国内研究者把握国际进展，寻找创新切入点。另一方面，具体应用探索也在进行中。例如，产业界的研究（如中兴通讯的技术报告）尝试将“大语言模型+蒙特卡洛树搜索”应用于通信网络故障的根因分析[40]。他们设计了基于LLM推理和MCTS的系统来排查复杂网络故障，利用LLM生成可能的故障原因作为节点，MCTS 搜索验证这些假设，从而提高故障诊断的准确率[40]。这一案例表明，国内对将LLM智能体用于实际问题抱有浓厚兴趣，正在探索军工、安全、通信等领域的落地可能。
总体来看，国内研究在LLM智能体领域起步虽晚但发展迅速。通过引入检索增强和强化学习优化等特色思路，国内学者提出了一批具有创新性的框架，如AR-MCTS、Tree-GRPO等，在国际上引起关注。同时，依托于国内庞大的应用需求和数据资源（如中文问答、多人对话、专业考试等场景），这些研究也具有自身的优势。例如，AR-MCTS 针对多模态和中文复杂问答进行了优化，填补了某些细分领域的空白。从发表渠道看，国内研究成果正逐步走向高水平国际会议（ACL、NeurIPS 等）和期刊，这表明国内在该领域的话语权正在提升。
然而也应看到，目前国内在顶尖工作数量和影响力上与国际领先水平尚存差距。一些核心理念（如ReAct、ToT、Reflexion等）最初仍来自海外团队，国内多数研究属于对前沿方法的跟进和改进。在大模型底层架构上，国内模型（如清华的ChatGLM、百度的文心大模型等）为开展LLM智能体研究提供了平台，但相比OpenAI的GPT-4等，模型能力差距可能限制了部分研究的效果。此外，在通用评测基准和开源平台构建上，国内尚缺乏具有国际影响力的成果，这些都是未来需要努力的方向。

## 关键差距
综合上述分析，可以看出当前国内外在LLM智能体树搜索领域存在一些差距和挑战：
1.	核心算法与框架的成熟度：国外已经提出了多个具有开创性的框架（如ReAct、LATS、Reflexion等），形成了多样化的方法谱系。相较之下，国内虽然有AR-MCTS等工作，但整体上原创性框架较少，多数方法仍处在验证可行性的阶段。在算法细节打磨和通用框架构建上，国内与国际顶尖水平仍有一定距离。
2.	模型与资源：一些国外研究直接基于最先进的模型（如GPT-4）进行实验，借助其强大的基础能力验证新方法的上限[11]。反观国内，受制于模型能力，目前大多使用开源模型或本土模型进行研究，模型性能差距可能导致新方法的实际增益不如预期。与此同时，国外拥有更丰富的高质量开放环境和基准（如AlfWorld、MiniWoB、HotpotQA等），而国内在构建贴合本土应用的评测环境方面才刚起步。
3.	研究视角与应用领域：国外研究视角更加多元，既关注学术难题（数学推理、代码生成），也面向实际场景（网页导航、机器人控制）。国内研究目前主要聚焦于问答和多模态等领域，应用探索相对有限。尤其在物理环境中的具身智能体、复杂交互任务等方面，国内报道的工作较少，这可能成为下一步需要拓展的方向。
4.	人才与合作：顶尖的LLM智能体研究往往是跨学科团队合作的结果，包括NLP、强化学习、知识检索、人机交互等领域专家。国内这一交叉团队的培养和协作机制尚在形成过程中，相比之下，国际上已有多机构协作的范例（如前述包含多所名校和实验室参与的Agentic RL综述[41]）。加强国内不同研究方向的融合，将有助于催生更具影响力的成果。
5.	实验规模和 reproducibility：国际领先工作通常在多个数据集、多种设定下验证方法的有效性，并公开代码方便社区复现。如LATS在HotpotQA、SQuAD、TriviaQA等上全面评估[18]。国内部分研究由于资源限制，实验规模偏小，通用性证明不充分；加之有的实现未开源，影响了后续跟进研究的开展。
综上所述，国内外在 LLM 智能体树搜索技术上既有共同的进步方向，也存在一定差距。国外研究占据先发优势，在理论和方法上呈现百花齐放之势；国内研究则后来居上，在特定领域实现突破。对于本课题而言，正是基于以上差距识别出潜在的创新切入点：我们计划充分借鉴国外先进方法（如 LATS 的树搜索思想）并结合国内优势（如多模态处理和领域数据），创新性地提出改进的 LLM 智能体树搜索方案，以缩小这一差距。在后续研究中，我们将针对上述薄弱环节（如模型优化、高效评测等）制定技术攻关路线，力求在该领域实现从跟跑到并跑甚至领跑的跨越。通过本课题的研究，有望为提高 LLM 智能体自主决策能力提供新的理论和技术支撑，进一步推动国内相关研究走向更高水平。[18][24]

# 2.研究目标，研究内容和关键技术

## 摘要

本课题面向复杂检索与多跳推理任务，提出语言智能体树搜索框架（Language Agent Tree Search, LATS），统一了语言模型的“推理—行动—规划”三类能力。在方法层面，LATS以树搜索为核心，结合上置信界（UCT）与蒙特卡洛树搜索（MCTS）两种扩展策略，并引入树结构差分奖励（Tree-GRPO）实现稳定高效的信用分配与批量扩展。体系结构上，框架在迭代预算受限的场景下显著提升成功率与样本效率；工程上，构建了可复现实验管线、自动评估与可视化工具。基于HotpotQA、TriviaQA与SQuAD等数据集的系统实验与消融对比，LATS在固定预算下相对ReAct与无树搜索基线取得稳定提升，验证了树结构差分奖励与规划式搜索对语言智能体的增益与鲁棒性。

## 研究目标（≤500字）

针对多源信息融合与长链条决策的复杂问答等场景，突破语言智能体在有限预算下的高效规划与稳定信用分配技术瓶颈，研制“语言智能体树搜索框架（LATS）”与“树结构差分奖励（Tree-GRPO）”两项核心成果；达到在HotpotQA/TriviaQA/SQuAD等数据集上，相对ReAct与无树搜索基线的显著成功率提升与更低平均成功迭代；具备统一接入UCT/MCTS扩展、批量树扩展与差分奖励的能力；取得在固定迭代/令牌预算下的更优样本效率、鲁棒性与可复现性；提升复杂推理任务的规划质量与收敛稳定性；实现端到端评估与可视化应用，为后续在信息检索、工具调用与通用代理任务的部署奠定基础。

## 研究内容

本课题的研究内容围绕“在受限预算下提升语言智能体的规划能力与信用分配稳定性”这一核心科学问题展开，涵盖方法论设计、算法实现、系统工程、数据与评估、以及消融验证五个方面，形成理论与工程闭环。

首先，在方法论层面，我们提出语言智能体树搜索（LATS）框架以统一“推理—行动—规划”。在每一轮迭代中，语言模型基于当前状态生成若干候选思路与动作，系统将其结构化为树节点，进行扩展、评估与回传。与序列式ReAct不同，LATS在同一迭代里并行探索多条候选路径，通过节点选择策略控制搜索宽度与深度，从而在不增加总体预算的前提下提高成功概率与样本效率。

其次，针对节点选择与扩展策略，我们分别研究UCT与MCTS两种机制。UCT通过上置信界平衡探索与利用：在统计累计回报与访问次数的同时引入对不确定性的惩罚/激励，优先选择既有良好历史表现又未被过度访问的路径；为适配语言模型输出的高噪声与非独立采样特性，我们引入动态探索系数与上下文敏感折扣，防止个别高分文本片段导致的过拟合。MCTS则在扩展阶段加入模拟与价值评估，通过短程rollout或价值函数估计预测未来收益，再将结果沿路径回传。两者在框架内可互换或混合使用：在浅层以UCT快速筛选，在深层以MCTS稳健规划，从而兼顾响应速度与全局质量。

再次，为解决并行扩展下的高方差与误导回传问题，我们提出树结构差分奖励（Tree-GRPO）。其核心思想是在同一批次、同一深度的候选节点间进行相对比较，以“差分优势”而非绝对分数指导信用分配：对每棵树的兄弟节点计算归一化回报并构造相对优势，抑制偶然高分样本的影响，提升梯度指向的稳定性。该机制与批量扩展天然契合，可在每一轮迭代进行向量化处理，降低计算成本并提高并发效率。实验显示，Tree-GRPO在不同数据集与不同预算下均能显著降低方差并加速收敛。

在系统工程方面，我们实现了可复现实验管线与健壮的运行时。`run_hotpot_experiment.py`统一管理数据加载、预算设定、模型配置与结果存储；`lats_tree_search_solver.py`封装树搜索求解器，支持UCT/MCTS与Tree-GRPO的组合；`utils/llm_client.py`提供与DeepSeek/OpenAI/Qwen的解耦接口与缓存策略，内置错误恢复与格式校验；所有实验均记录完整配置（包含`tree_expand_mode`、`tree_reward_mode`、`mcts_num_simulations`等），以JSON形式保存，保证结果可复核与复现。在可视化方面，构建了热力图（模型×数据集成功率）、模式比较柱状图、预算—成功率趋势线与迭代分布箱线等，支持跨数据集、跨变体的系统性分析。

数据与任务设置选取HotpotQA（多跳事实推理）、TriviaQA与SQuAD（阅读理解）三类代表性数据集，覆盖从多源信息整合到证据检索的不同难度层级。我们采用Dev/Test分割与顺序/随机采样两种加载模式，并通过随机种子控制保证可复现实验。预算方面统一限定最大迭代次数与令牌消耗上限，确保不同方法与变体在相同资源约束下公平对比。

评估与分析侧重于四类主要指标：成功率、平均成功迭代、样本效率（成功数/预算）、鲁棒性（不同预算下的累计成功曲线）；辅以计算成本（令牌与时间）、通量（吞吐量）、失败类型与稳定性统计。在消融实验中，我们系统考察扩展分支数、模拟次数、价值函数/rollout的使用与差分奖励开关等关键因素，并通过跨数据集一致性与统计显著性检验验证结论的稳健性。

最后，我们对真实API与演示模式进行了统一适配：在网络受限或成本受限环境下，可通过演示生成器产生结构化的历史记录与结果分布，用以校验可视化与管线正确性；在真实环境中，系统通过异常捕获与回退路径保证长时间运行的稳定性，为后续部署到工具调用与检索代理任务提供工程基础。

## 关键技术

统一的树搜索推理框架（LATS）是本课题的核心，它将语言模型产生的思维链与行动序列显式结构化为树形搜索空间。在每次迭代中，系统从当前状态出发生成多条候选路径，并以节点形式并行扩展；随后对节点进行评估（基于得分、价值与一致性），并将评估结果沿路径向上回传，形成对搜索方向的全局调控。与线性推理不同，树搜索能够在有限预算下同时覆盖多种推理假设，显著提高找到正确解的概率。

树结构差分奖励（Tree-GRPO）解决了批量扩展带来的信用分配不稳定问题。传统绝对回报在语言模型高噪声输出下容易受偶然高分样本误导，导致梯度方向不稳定。Tree-GRPO通过对兄弟节点构造相对优势（差分回报），将奖励中心化与归一化，削弱异常值影响；在并行场景下可按批向量化计算，显著降低方差并提升收敛速度。该技术与UCT/MCTS均兼容，可在不同深度与不同批量策略下工作。

UCT/MCTS扩展策略的融合使得框架既具备快速筛选能力，又具备深层规划能力。UCT利用访问统计与不确定性项平衡探索/利用，适用于浅层快速排除低质量路径；MCTS通过模拟与价值评估提供对未来收益的稳健估计，适用于深层推理与长链条任务。我们在框架中提供两者的统一接口与调度策略，使其能够按任务难度与预算动态切换或混合。

预算约束与鲁棒性控制贯穿系统实现。我们在配置层面统一管理迭代与令牌预算，提供早停与成功检测；在运行时层面内置错误恢复、格式校验与重试回退路径，保证长时间运行的稳定性与结果可复核性。该能力确保不同方法在相同约束下公平对比，也为工程化部署提供稳定基础。

可复现实验管线与可视化工具提供了从数据加载、配置记录、结果评估到图表生成的完整支撑。结果以JSON结构化存储，包含模型、预算、扩展模式与奖励模式等关键元数据；可视化模块生成跨模型×数据集的热力图、模式比较与预算—成功率趋势线，并对迭代分布进行统计展示。该工具链既支撑论文图表的自动化生成，也方便第三方复核与复现实验。
