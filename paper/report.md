# 1.课题需求与国内外研究状况
## 课题需求
随着大型语言模型（LLM）的快速发展，它们在文本生成、问答等任务上取得了引人注目的成果。然而，现有的主流 LLM 缺乏真实的行动能力和规划能力：当模型面临需要多步骤推理、计划决策、工具调用或环境交互的复杂任务时，往往显得力不从心[1]。例如，在传统搜索和问答场景中，LLM 通常以左到右的链式推理方式工作，一步步生成答案。但这种线性推理模式在遇到复杂问题时容易陷入局部最优，缺乏全局探索能力。这导致模型可能产生幻觉或错误结论，无法保证推理过程的可靠性。
当前广泛采用的“检索增强生成”（RAG）等方案在一定程度上提升了信息获取效率，但它们的交互模式通常是静态单轮的[2]。LLM 在其中主要扮演“优化者”或“总结者”的角色，难以应对需要多步骤、动态调整的复杂调研任务[2]。因此，业界和学界认识到仅靠被动的单轮问答已无法满足更高层次智能体的需求，迫切需要让 LLM 从“回答问题”进化为“自主执行任务”[3]。具体而言，这意味着赋予模型以下能力：将任务分解为子目标并规划多步行动，在每一步决策中利用环境反馈进行调整，并能调用外部工具或检索外部知识来辅助决策。这种转变对于构建具备自主性的智能代理至关重要，是当前人工智能研究中最具挑战性的方向之一[3][4]。
为了解决上述需求，研究者开始探索将经典的决策搜索算法引入到 LLM 的推理过程中。其中，蒙特卡洛树搜索（MCTS）作为一种成熟的启发式搜索方法，因其在博弈决策领域的成功而备受关注。例如，DeepMind 团队的 AlphaGo 系统结合深度神经网络和 MCTS，在围棋任务上达到了划时代的表现，首次击败了人类职业选手[5]。MCTS 通过在状态空间中模拟多轮决策，能够有效平衡探索与利用，在复杂决策环境中找到高质量方案。因此，一个合理的设想是：将 MCTS 等搜索策略与 LLM 相结合，让语言模型具备类似游戏AI的规划搜索能力，从而能够在解答复杂问题时探索多条推理路径，避免过早收敛到错误答案[6][7]。这种方法有望显著提升 LLM 在推理任务中的可靠性和成功率。总之，从应用需求看，无论是在学术问题（如数学推导、复杂问答）还是实际场景（如自动驾驶决策、指挥控制等复杂任务）中，都迫切需要一种融合语言智能和搜索规划的新型方法，以突破当前 LLM 智能体在推理决策方面的瓶颈。
## 国外研究现状
在国际上，围绕LLM智能体推理与规划的研究近两年蓬勃兴起，涌现出一系列创新方法和框架。早期的工作侧重于增强语言模型的链式推理能力，例如提出思维链（Chain-of-Thought）提示技术，让模型在给出最终答案前生成一系列中间推理步骤，从而提高复杂问题的解答准确性[6]。基于此思路，自洽性推理（Self-Consistency）方法进一步通过对同一问题采样生成多条思维链并投票，来提升答案可靠度[8]。然而，这些方法仍局限于线性推理，无法在单次推理过程中灵活地探索不同路径。
2022年底，来自普林斯顿等机构的研究者提出了ReAct范式[9]。ReAct将“思考”和“行动”相结合，把LLM的推理过程显式地表示为交替的“Thought-Action-Observation”（思考-行动-观察）序列[9]。具体而言，模型首先生成思考内容（如推理分析），然后根据思考选择行动（如调用工具或检索知识），接着获得环境观察结果并反馈到下一步思考中。通过这种循环，ReAct实现了LLM与外部环境的互动，在开放域问答、知识检索等任务中显著提升了性能[9]。ReAct的优势在于：它赋予了模型一定的计划执行能力和可解释性，能够通过工具调用扩充模型的知识范围[9]。但其局限也很明显：ReAct采用的是贪心式的线性搜索策略，每次只沿着当前认为最优的路径向前推进，缺乏全局搜索视野。这使得模型可能在决策过程中过早地陷入某一路径，一旦该路径不能通向正确答案，就难以及时回溯和探索其他可能方向。
针对ReAct的局限，学者们开始尝试为LLM智能体引入非线性的决策探索机制。树形搜索理念逐渐进入视野：2023年，Princeton团队提出了思想树(Tree of Thoughts, ToT)方法[6]。ToT将Chain-of-Thought推广为树状结构，即模型在每个推理步骤产生多个候选“想法”（thought），形成分支，然后通过评估或启发式选择最优分支继续推进[10]。这种方法允许模型同时探索多种推理路线，并可在必要时进行回溯和重新选择，从而显著增强了解题的规划性。实验表明，Tree of Thoughts 能够大幅提升复杂推理任务的成功率。例如，在24点算术游戏中，GPT-4 结合思维树策略仅能解出4%的题目，而使用ToT框架后成功率飙升至74%[11]。这一结果充分证明了树式思维对提升LLM决策能力的价值。
与ToT类似，2023年出现的Reflexion框架引入了自我反思机制[12][13]。Reflexion并非直接采用搜索算法，而是让智能体在每轮尝试后根据反馈生成反思，将错误教训以语言形式存入记忆池，指导后续回合的决策[12][13]。通过这种语言层面的强化学习（不更新模型权重，而是靠文本反馈），Reflexion在多个任务上显著提高了LLM的成功率。例如，在HumanEval编程基准上，Reflexion使得GPT-4的单次通过率从80%提升到91%[14]。Reflexion展示了利用语言反馈实现强化决策的前景，与搜索方法形成了互补：前者强调通过多次试错学习策略，后者强调在一次推理中广泛探索。
在将搜索算法直接用于LLM推理方面，最具代表性的是语言智能体树搜索（Language Agent Tree Search, LATS）方法。Zhou 等人于2023年底在arXiv发表了相关工作[15]。LATS 可视作将 MCTS 完全集成到LLM智能体中的首个框架：它把LLM的思考-行动序列构造成一棵搜索树，树节点表示智能体在某一步的状态（包含累积的思考、行动和观察序列），树边表示可选的行动。在决策过程中，LATS 执行典型的MCTS四阶段：选择最优节点、扩展候选动作、由LLM对候选进行评估得到价值、然后模拟并回溯更新[16]。通过上置信上限(UCB)等策略，LATS 能够在庞大的决策空间中统计指导搜索方向，避免了盲目探索。与ReAct只能沿一条路走到黑不同，LATS 并行探索多个推理路径，大大降低了错失正确答案路径的风险[16]。此外，LATS 还融入了自我反思步骤：对于每个候选路径，模型可以结合自身观察和反馈，对推理过程进行检查，一旦发现谬误便尝试替代方案，从而提升决策的稳健性[17]。
实验研究表明，LATS 在一系列复杂推理任务中取得了优于传统方法的效果。以多跳问答为例，在 HotpotQA 等数据集上，相较于 ReAct 框架，LATS 在答案精准率和任务成功率上均有显著提升[18]。又比如在代码生成、交互式问答等需要多步决策的场景，LATS 能够更有效地规划步骤并避免错误[18]。需要指出的是，LATS 的高性能是以一定的计算代价为前提的：由于引入了树搜索和反思评估，计算开销和时间消耗较 ReAct 等线性方法有所增加[18]。然而在许多关键任务中，这一代价是值得的，因为 LATS 换来了更强的推理泛化能力和可靠性[18]。
除上述方法外，国际上还有一些相关探索值得关注。例如，Wang 等人提出的Tree-of-Thoughts 改进版引入了步骤级的Q值评估，通过强化学习训练模型学会评估每个推理步骤的价值，从而指导树搜索更高效地收敛[19]。OpenAI 在 GPT-4 的系统卡中也提出了一种“慢思考”机制，结合MCTS策略和验证模型来模拟人类的深思熟虑过程，提高推理准确性[20]。另外，2024年出现的R-Search框架[21]将检索式搜索与LLM推理相集成，通过多重奖励的强化学习使模型能在何时检索、何时推理之间动态决策，进一步提升了知识密集型任务中的推理表现[22]。总体而言，国外研究在过去两年中已经初步构建了LLM自主智能体的雏形，从利用环境反馈的ReAct到树搜索规划的LATS，再到自我改进的Reflexion，呈现出多方向并进的局面。这些方法大多在国际顶会上发表或报告（如 NeurIPS 2023、ICLR 2024 等），标志着LLM智能体正迅速成为人工智能领域的新热点。
值得一提的是，国外学者还开始对这一新兴领域进行系统性综述和评估框架的搭建。例如，2023年底的一篇综述详细分类了基于LLM的自主代理研究，涵盖了推理、规划、工具调用等模块[15]。2024年4月也有研究发布了关于AI Agent架构的综述，分析了单智能体和多智能体系统中的关键模式和差异[23]。2025年更有学者联合16家机构撰写了长达百页的Agentic RL综述，整合了500多篇相关工作，对具身智能体强化学习的概念、框架和应用进行了系统梳理[24]。这些综述工作的出现，表明国际研究者正试图提炼共性原理，指导未来研究。从综述观点看，一个共识是：未来的LLM智能体应当集成规划(Planning)、记忆(Memory)、工具使用(Tool Use)、推理(Reasoning)、自我反思(Self-Reflection)等多种能力[25][26]，通过强化学习等途径提升模型的自主决策水平[4]。

## 国内研究现状
近年来，国内研究者也紧跟国际前沿，在LLM智能体与树搜索增强方面开展了一系列工作。在总体思路上，国内研究与国际趋势一脉相承，同样关注如何让大型语言模型具备自主规划、多步推理的能力。一些中国团队在国际顶会上发表了相关成果，体现出不俗的竞争力。
首先，值得关注的是国内学者在将树搜索与LLM相结合方面所做的探索。中国人民大学等单位的研究团队提出了AR-MCTS框架[27]（已被ACL 2025接收）。AR-MCTS 的设计初衷是提升多模态大模型（MLLM）的复杂推理能力[28]。该框架巧妙地将主动检索（Active Retrieval, AR）与蒙特卡洛树搜索融合：在每一步推理中，智能体不仅利用自身语言模型产生候选推理步骤，还通过检索模块主动获取与当前问题相关的跨模态知识[28][27]。然后，AR-MCTS 使用 MCTS 策略在“候选步骤+检索信息”的扩展状态下进行模拟和评估，选出最优的下一步推理决策。通过不断迭代，AR-MCTS 能逐步产生高质量的多模态推理路径，并利用所累积的数据对过程奖励模型（PRM）进行强化训练，从而提升模型对复杂问题的理解和验证能力[29][30]。实验结果显示，AR-MCTS 框架在跨模态推理任务上显著优于传统方法，如在综合性高考多模态基准（GAOKAO-MM）上，相比零样本GPT-4V等模型取得了更高的准确度[31]。AR-MCTS 是国内学者将检索、推理、搜索三者相结合的一次成功实践，表明通过引入知识检索和树搜索可以有效增强大型模型在复杂场景下的推理表现。
除了在多模态领域的进展，国内研究者也在单模态的LLM智能体树搜索上有所贡献。微软亚洲研究院的团队提出了Tree-of-Thoughts 强化学习优化(TREE-GRPO)方法[32]。该方法针对LLM智能体在交互式环境中的决策优化问题，利用树搜索结构来提高强化学习的采样效率和稳定性。据报道，Tree-GRPO通过在训练过程中维护一棵搜索树来估计策略的价值，并使用树中节点的信息来指导策略梯度更新，从而相比传统策略优化方法获得更好的收敛效果[33]。虽然Tree-GRPO主要作为训练算法提出，但它与推理时的LATS框架理念相辅相成：前者优化了智能体的学习过程，后者提升了智能体的推理过程，两者结合有望进一步提高LLM智能体的性能。值得一提的是，该工作最初以预印本形式发表（2024年初[15]），其后续扩展在2025年继续完善[34]。这表明国内科研力量在这一领域正持续投入，并有能力引领某些技术方向。
在智能体推理的评测与应用方面，国内也开始逐步布局。一方面，综述和评论性工作已在国内出现，以促进学术交流。比如，上海交通大学团队于2025年发布了首篇LLM 搜索智能体综述[35]，系统分析了搜索智能体的范式、优化、应用和评估四个维度，为该领域提供了清晰的研究路线图[36][37]。该综述强调了从传统检索到自主搜索智能体的范式演变，指出未来搜索将从人工驱动转向AI主动信息获取的新阶段[38][39]。这种工作有助于国内研究者把握国际进展，寻找创新切入点。另一方面，具体应用探索也在进行中。例如，产业界的研究（如中兴通讯的技术报告）尝试将“大语言模型+蒙特卡洛树搜索”应用于通信网络故障的根因分析[40]。他们设计了基于LLM推理和MCTS的系统来排查复杂网络故障，利用LLM生成可能的故障原因作为节点，MCTS 搜索验证这些假设，从而提高故障诊断的准确率[40]。这一案例表明，国内对将LLM智能体用于实际问题抱有浓厚兴趣，正在探索军工、安全、通信等领域的落地可能。
总体来看，国内研究在LLM智能体领域起步虽晚但发展迅速。通过引入检索增强和强化学习优化等特色思路，国内学者提出了一批具有创新性的框架，如AR-MCTS、Tree-GRPO等，在国际上引起关注。同时，依托于国内庞大的应用需求和数据资源（如中文问答、多人对话、专业考试等场景），这些研究也具有自身的优势。例如，AR-MCTS 针对多模态和中文复杂问答进行了优化，填补了某些细分领域的空白。从发表渠道看，国内研究成果正逐步走向高水平国际会议（ACL、NeurIPS 等）和期刊，这表明国内在该领域的话语权正在提升。
然而也应看到，目前国内在顶尖工作数量和影响力上与国际领先水平尚存差距。一些核心理念（如ReAct、ToT、Reflexion等）最初仍来自海外团队，国内多数研究属于对前沿方法的跟进和改进。在大模型底层架构上，国内模型（如清华的ChatGLM、百度的文心大模型等）为开展LLM智能体研究提供了平台，但相比OpenAI的GPT-4等，模型能力差距可能限制了部分研究的效果。此外，在通用评测基准和开源平台构建上，国内尚缺乏具有国际影响力的成果，这些都是未来需要努力的方向。

## 关键差距
综合上述分析，可以看出当前国内外在LLM智能体树搜索领域存在一些差距和挑战：
1.	核心算法与框架的成熟度：国外已经提出了多个具有开创性的框架（如ReAct、LATS、Reflexion等），形成了多样化的方法谱系。相较之下，国内虽然有AR-MCTS等工作，但整体上原创性框架较少，多数方法仍处在验证可行性的阶段。在算法细节打磨和通用框架构建上，国内与国际顶尖水平仍有一定距离。
2.	模型与资源：一些国外研究直接基于最先进的模型（如GPT-4）进行实验，借助其强大的基础能力验证新方法的上限[11]。反观国内，受制于模型能力，目前大多使用开源模型或本土模型进行研究，模型性能差距可能导致新方法的实际增益不如预期。与此同时，国外拥有更丰富的高质量开放环境和基准（如AlfWorld、MiniWoB、HotpotQA等），而国内在构建贴合本土应用的评测环境方面才刚起步。
3.	研究视角与应用领域：国外研究视角更加多元，既关注学术难题（数学推理、代码生成），也面向实际场景（网页导航、机器人控制）。国内研究目前主要聚焦于问答和多模态等领域，应用探索相对有限。尤其在物理环境中的具身智能体、复杂交互任务等方面，国内报道的工作较少，这可能成为下一步需要拓展的方向。
4.	人才与合作：顶尖的LLM智能体研究往往是跨学科团队合作的结果，包括NLP、强化学习、知识检索、人机交互等领域专家。国内这一交叉团队的培养和协作机制尚在形成过程中，相比之下，国际上已有多机构协作的范例（如前述包含多所名校和实验室参与的Agentic RL综述[41]）。加强国内不同研究方向的融合，将有助于催生更具影响力的成果。
5.	实验规模和 reproducibility：国际领先工作通常在多个数据集、多种设定下验证方法的有效性，并公开代码方便社区复现。如LATS在HotpotQA、SQuAD、TriviaQA等上全面评估[18]。国内部分研究由于资源限制，实验规模偏小，通用性证明不充分；加之有的实现未开源，影响了后续跟进研究的开展。
综上所述，国内外在 LLM 智能体树搜索技术上既有共同的进步方向，也存在一定差距。国外研究占据先发优势，在理论和方法上呈现百花齐放之势；国内研究则后来居上，在特定领域实现突破。对于本课题而言，正是基于以上差距识别出潜在的创新切入点：我们计划充分借鉴国外先进方法（如 LATS 的树搜索思想）并结合国内优势（如多模态处理和领域数据），创新性地提出改进的 LLM 智能体树搜索方案，以缩小这一差距。在后续研究中，我们将针对上述薄弱环节（如模型优化、高效评测等）制定技术攻关路线，力求在该领域实现从跟跑到并跑甚至领跑的跨越。通过本课题的研究，有望为提高 LLM 智能体自主决策能力提供新的理论和技术支撑，进一步推动国内相关研究走向更高水平。[18][24]

# 2.研究目标，研究内容和关键技术

## 摘要

本课题面向复杂检索与多跳推理任务，提出语言智能体树搜索框架（Language Agent Tree Search, LATS），统一了语言模型的“推理—行动—规划”三类能力。在方法层面，LATS以树搜索为核心，结合上置信界（UCT）与蒙特卡洛树搜索（MCTS）两种扩展策略，并引入树结构差分奖励（Tree-GRPO）实现稳定高效的信用分配与批量扩展。体系结构上，框架在迭代预算受限的场景下显著提升成功率与样本效率；工程上，构建了可复现实验管线、自动评估与可视化工具。基于HotpotQA、TriviaQA与SQuAD等数据集的系统实验与消融对比，LATS在固定预算下相对ReAct与无树搜索基线取得稳定提升，验证了树结构差分奖励与规划式搜索对语言智能体的增益与鲁棒性。

## 研究目标（≤500字）

针对多源信息融合与长链条决策的复杂问答等场景，突破语言智能体在有限预算下的高效规划与稳定信用分配技术瓶颈，研制“语言智能体树搜索框架（LATS）”与“树结构差分奖励（Tree-GRPO）”两项核心成果；达到在HotpotQA/TriviaQA/SQuAD等数据集上，相对ReAct与无树搜索基线的显著成功率提升与更低平均成功迭代；具备统一接入UCT/MCTS扩展、批量树扩展与差分奖励的能力；取得在固定迭代/令牌预算下的更优样本效率、鲁棒性与可复现性；提升复杂推理任务的规划质量与收敛稳定性；实现端到端评估与可视化应用，为后续在信息检索、工具调用与通用代理任务的部署奠定基础。

## 研究内容

本课题围绕语言智能体在复杂推理任务中的规划能力与样本效率问题，开展以下三个方面的深入研究：

**1. 语言智能体树搜索（LATS）机制研究**
针对大语言模型在长链条推理中易产生幻觉与迷失方向的问题，研究引入蒙特卡洛树搜索（MCTS）作为智能体的“思维导航器”。具体研究如何将“推理—行动—规划”统一在树结构中，利用 UCT（Upper Confidence Bound for Trees）算法平衡探索与利用，设计基于价值函数的节点评估机制。通过在虚拟环境中进行模拟（Rollout）与反向传播，使智能体在执行真实动作前具备预演和自我纠错能力，从而实现从“直觉式反应”向“规划式思考”的范式转变。

**2. 树结构差分奖励（Tree-GRPO）算法设计**
针对稀疏奖励环境下信用分配困难的问题，研究基于树结构的组相对策略优化（Group Relative Policy Optimization）算法。改变传统单轨迹优化的模式，构建并行多树（Batch Trees）探索机制，一次性初始化 $m$ 棵搜索树进行竞争式生长。研究树结构差分奖励的计算方法，通过比较同一父节点下不同子分支的相对优势，计算细粒度的优势函数（Advantage），从而在缺乏中间奖励的任务中实现稳定的梯度指引，降低策略更新的方差。

**3. MCTS 引导的并行树搜索框架融合研究**
研究如何将 LATS 的精细化规划能力与 Tree-GRPO 的并行探索能力进行有机融合。构建“MCTS 引导 + 批量执行”的双层架构：在微观层面，利用 LATS 的价值函数为每一棵树的节点扩展提供高质量的候选动作指导，避免盲目搜索；在宏观层面，利用 Tree-GRPO 的并行机制维护多样的搜索路径，防止陷入局部最优。通过真实环境反馈与 MCTS 价值估计的闭环交互，实现智能体在有限预算下推理成功率与样本效率的双重提升。

## 关键技术

**1. MCTS 增强的语言智能体规划技术**
该技术突破了传统 ReAct 框架单向链式推理的局限。核心在于构建一个动态生长的推理树，其中每个节点代表一个“思考-行动”状态。利用大语言模型作为价值函数（Value Function），对未探索的节点进行前瞻性打分（0-10分）；结合 UCT 算法动态选择最具潜力的路径进行扩展。技术难点在于设计高效的剪枝策略与模拟机制，确保在有限的 Token 预算内，智能体能够通过模拟预演排除错误路径，显著提升复杂多跳问答的决策质量。

**2. 树结构差分奖励分配机制（Tree-GRPO）**
该技术解决了长视距任务中“好动作被埋没、坏动作被奖励”的信用分配难题。关键在于引入“相对优势”概念，不再仅仅依赖最终的二值化奖励（成功/失败）。技术实现上，系统维护一组并行的搜索树，在每一轮迭代中，对比兄弟节点的子树价值，计算差分奖励（Differential Reward）。这种机制能够从失败的轨迹中提取有用的局部信息，或在成功的轨迹中识别关键的转折步骤，从而为策略模型提供更平滑、更准确的优化信号。

**3. 并行树搜索与稀疏奖励增强框架**
这是本课题的系统级集成创新。框架采用“主从协同”设计：主循环控制 Tree-GRPO 的批量迭代与资源调度，子模块运行 LATS 的 MCTS 选择与反向传播。关键流程包括：(1) **并行初始化**：同时启动 $m$ 个推理进程；(2) **引导式扩展**：利用 MCTS 评估指导每棵树生成 $n$ 个最优候选步；(3) **批量交互**：将候选步批量提交至环境执行，获取真实反馈；(4) **混合回传**：将环境的硬奖励（Reward）与价值函数的软评分（Value）结合，更新树结构统计量。该框架实现了探索广度（Tree-GRPO）与挖掘深度（LATS）的数学统一。
